"""
QUICK REFERENCE: Azure PostgreSQL Integration
===============================================

This guide shows you how to integrate terminal output logging with Azure PostgreSQL
into your existing SecureGuard backend architecture.

FILE HIERARCHY (Your Current Structure)
========================================

K/
├── Iteration_1/
│   └── backend/
│       ├── main.py                           ← ADD db endpoints here
│       ├── ml_model.py                       ← Your existing ML detector
│       ├── cnn_client.py                     ← CNN service client
│       ├── vt_integration.py                 ← VirusTotal enrichment
│       ├── .env                              ← ADD DATABASE_URL here
│       ├── requirements.txt                  ← ADD sqlalchemy, asyncpg
│       │
│       ├── db_manager.py                     ← NEW: Copy from examples/
│       ├── terminal_logger.py                ← NEW: Copy from examples/
│       │
│       └── examples/                         ← Reference implementations
│           ├── db_manager_example.py
│           ├── terminal_logger_example.py
│           └── TERMINAL_OUTPUT_DB_EXAMPLE.md


IMPLEMENTATION CHECKLIST
=========================

[ ] 1. Set up Azure PostgreSQL
    - Go to Azure Portal
    - Create PostgreSQL Flexible Server
    - Note: server name, admin username, password, database name
    - Configure firewall to allow your IP
    - Enable SSL connection

[ ] 2. Add Database URL to .env
    ```bash
    DATABASE_URL=postgresql+asyncpg://username:password@servername.postgres.database.azure.com:5432/databasename?ssl=require
    ```

[ ] 3. Install Dependencies
    ```bash
    cd Iteration_1/backend
    pip install sqlalchemy[asyncio] asyncpg python-dotenv
    ```

[ ] 4. Copy Example Files
    ```bash
    # Copy from examples/ to backend/
    cp examples/db_manager_example.py db_manager.py
    cp examples/terminal_logger_example.py terminal_logger.py
    ```

[ ] 5. Update main.py
    Add these imports at the top:
    ```python
    from sqlalchemy.ext.asyncio import AsyncSession
    from fastapi import Depends
    from db_manager import init_db, get_session, save_terminal_log, save_scan_history
    from terminal_logger import LoggingCapture, format_scan_output
    ```
    
    Add database init to startup_event():
    ```python
    @app.on_event("startup")
    async def startup_event():
        # ... existing code ...
        
        # Initialize database
        try:
            logger.info("Initializing database...")
            await init_db()
            logger.info("✓ Database ready")
        except Exception as e:
            logger.warning(f"Database init failed: {e}")
    ```
    
    Add db session to scan endpoints:
    ```python
    @app.post("/scan", response_model=ScanResponse)
    async def scan_file(
        request: ScanRequest,
        db: AsyncSession = Depends(get_session)  # ADD THIS
    ):
        # ... existing scan logic ...
        
        # After scan completes, save to DB:
        try:
            await save_scan_history(
                session=db,
                file_path=request.file_path,
                result=result,
                model_type="CNN" if cnn_detector else "Traditional ML"
            )
        except Exception as db_error:
            logger.error(f"Failed to log to DB: {db_error}")
    ```

[ ] 6. Test Database Connection
    ```bash
    python -c "from db_manager import init_db; import asyncio; asyncio.run(init_db())"
    ```

[ ] 7. Test Full Integration
    - Start backend: `python main.py`
    - Scan a file: Use browser extension or POST to /scan
    - Check database: Query /logs/scans endpoint
    - Verify Azure Portal: Check tables were created


EXECUTION FLOW DIAGRAM
=======================

┌─────────────────┐
│ Browser Ext     │
│ (frontend)      │
└────────┬────────┘
         │
         │ POST /scan {"file_path": "malware.exe"}
         ↓
┌────────────────────────────────────────────────────┐
│ main.py (FastAPI Backend)                          │
│                                                     │
│  @app.post("/scan")                                │
│  async def scan_file(                              │
│      request: ScanRequest,                         │
│      db: AsyncSession = Depends(get_session) ←────┼───┐
│  ):                                                │   │
│                                                     │   │
│    ┌─────────────────────────────────────────┐    │   │
│    │ 1. Capture logging output               │    │   │
│    │    with LoggingCapture() as capture:    │    │   │
│    │        result = detector.scan_file()    │    │   │
│    └─────────────────────────────────────────┘    │   │
│              │                                     │   │
│              ↓                                     │   │
│    ┌─────────────────────────────────────────┐    │   │
│    │ 2. Get captured output                  │    │   │
│    │    output = capture.get_output()        │    │   │
│    └─────────────────────────────────────────┘    │   │
│              │                                     │   │
│              ↓                                     │   │
│    ┌─────────────────────────────────────────┐    │   │
│    │ 3. Save to database (ASYNC)             │    │   │
│    │    await save_terminal_log(...)         │────┼───┤
│    │    await save_scan_history(...)         │────┼───┤
│    └─────────────────────────────────────────┘    │   │
│              │                                     │   │
│              ↓                                     │   │
│    ┌─────────────────────────────────────────┐    │   │
│    │ 4. Return scan result                   │    │   │
│    │    return ScanResponse(...)             │    │   │
│    └─────────────────────────────────────────┘    │   │
└────────────────────────────────────────────────────┘   │
                                                          │
         ┌────────────────────────────────────────────────┘
         │
         ↓
┌──────────────────────────────────────────────┐
│ db_manager.py (Database Layer)               │
│                                               │
│  get_session() ──────────┐                   │
│                          │                   │
│  save_terminal_log() ────┼──→ Connection     │
│  save_scan_history() ────┼──→ Pool           │
│  get_recent_logs()   ────┼──→ (5 sessions)   │
│  get_scan_history()  ────┘                   │
└──────────────────┬────────────────────────────┘
                   │
                   │ SQL over SSL
                   ↓
┌──────────────────────────────────────────────┐
│ Azure PostgreSQL (Cloud Database)            │
│                                               │
│  Tables:                                      │
│  ┌────────────────────────────────────┐     │
│  │ terminal_logs                      │     │
│  ├────────────────────────────────────┤     │
│  │ id          SERIAL PRIMARY KEY     │     │
│  │ timestamp   TIMESTAMP              │     │
│  │ command     VARCHAR(500)           │     │
│  │ stdout      TEXT                   │     │
│  │ stderr      TEXT                   │     │
│  │ scan_result JSON                   │     │
│  └────────────────────────────────────┘     │
│                                               │
│  ┌────────────────────────────────────┐     │
│  │ scan_history                       │     │
│  ├────────────────────────────────────┤     │
│  │ id              SERIAL PRIMARY KEY │     │
│  │ timestamp       TIMESTAMP          │     │
│  │ file_name       VARCHAR(255)       │     │
│  │ is_malicious    BOOLEAN            │     │
│  │ confidence      FLOAT              │     │
│  │ vt_data         JSON               │     │
│  └────────────────────────────────────┘     │
└──────────────────────────────────────────────┘


DATA FLOW EXAMPLE
==================

1. File Scan Request:
   ```
   POST /scan
   {
     "file_path": "C:\\Downloads\\malware.exe",
     "enable_vt": true
   }
   ```

2. Backend Captures Output:
   ```python
   # Logs during scan:
   "INFO - Scanning file: malware.exe"
   "INFO - Extracting PE features..."
   "INFO - Model prediction: MALWARE (95%)"
   "INFO - Enriching with VirusTotal..."
   "INFO - VT detection: 45/70"
   ```

3. Saved to terminal_logs table:
   ```sql
   INSERT INTO terminal_logs (
     command, command_type, stdout, scan_result, execution_time_ms
   ) VALUES (
     'scan_file: malware.exe',
     'malware_scan',
     'INFO - Scanning file...\nINFO - Model prediction...',
     '{"is_malicious": true, "confidence": 0.95, ...}',
     123.45
   );
   ```

4. Saved to scan_history table:
   ```sql
   INSERT INTO scan_history (
     file_name, is_malicious, confidence, model_type, vt_detection_ratio
   ) VALUES (
     'malware.exe', true, 0.95, 'CNN', '45/70'
   );
   ```

5. Response to Browser:
   ```json
   {
     "is_malicious": true,
     "confidence": 0.95,
     "prediction_label": "MALWARE",
     "scan_time_ms": 123.45,
     "features_analyzed": 4096,
     "vt_data": {"detection_ratio": "45/70"}
   }
   ```


NEW API ENDPOINTS (After Implementation)
=========================================

GET /logs/recent?limit=50
- Returns recent terminal logs
- Response: {"total": 50, "logs": [...]}

GET /logs/scans?limit=100&malicious_only=true
- Returns scan history with filtering
- Response: {"total": 25, "scans": [...]}

GET /logs/stats
- Returns aggregate statistics
- Response: {"total_scans": 1234, "malware_detected": 56, ...}


QUERYING FROM AZURE PORTAL
===========================

After scans are logged, you can query from Azure Portal SQL editor:

-- Get all malware detections today
SELECT file_name, confidence, model_type, timestamp
FROM scan_history
WHERE is_malicious = true
  AND timestamp > CURRENT_DATE
ORDER BY confidence DESC;

-- Get scan performance metrics
SELECT 
    model_type,
    COUNT(*) as total_scans,
    AVG(scan_time_ms) as avg_scan_time,
    AVG(confidence) as avg_confidence
FROM scan_history
GROUP BY model_type;

-- Find files scanned multiple times
SELECT file_name, COUNT(*) as scan_count
FROM scan_history
GROUP BY file_name
HAVING COUNT(*) > 1
ORDER BY scan_count DESC;

-- Get terminal errors/failures
SELECT command, stderr, timestamp
FROM terminal_logs
WHERE success = false
ORDER BY timestamp DESC
LIMIT 20;


TROUBLESHOOTING
===============

Problem: "Could not connect to database"
Solution: 
  - Check DATABASE_URL in .env
  - Verify Azure firewall allows your IP
  - Test: ping servername.postgres.database.azure.com

Problem: "SSL connection required"
Solution:
  - Ensure DATABASE_URL ends with ?ssl=require
  - Azure PostgreSQL requires SSL by default

Problem: "Table does not exist"
Solution:
  - Run: await init_db() during startup
  - Check startup logs for "Database tables initialized"

Problem: "Scan works but nothing in database"
Solution:
  - Check logs for "Failed to log to database"
  - Verify db: AsyncSession = Depends(get_session) in endpoint
  - Ensure await save_scan_history() is called

Problem: "Database operations slow"
Solution:
  - Check Azure region (use same region as your server)
  - Increase connection pool size in db_manager.py
  - Add indexes on frequently queried columns


PERFORMANCE CONSIDERATIONS
===========================

1. Connection Pooling (Already configured):
   - pool_size=5 (concurrent connections)
   - max_overflow=10 (additional connections under load)

2. Async Operations:
   - All DB ops use async/await (non-blocking)
   - Scans don't wait for DB writes to complete
   - If DB fails, scan still returns result

3. Graceful Degradation:
   ```python
   try:
       await save_scan_history(...)
   except Exception as db_error:
       logger.error(f"DB logging failed: {db_error}")
       # Scan result still returned to user
   ```

4. Output Truncation:
   - Use truncate_output() from terminal_logger
   - Prevents huge logs from filling database
   - Max 10KB per log entry by default


SECURITY NOTES
==============

1. Never commit DATABASE_URL to git
   - Add .env to .gitignore
   - Use environment variables in production

2. Use SSL connections (required by Azure)
   - Always include ?ssl=require in DATABASE_URL

3. Sanitize file paths before logging
   - Don't log full system paths in production
   - Use Path(file_path).name for file_name column

4. Rotate connection credentials
   - Change Azure PostgreSQL password periodically
   - Update DATABASE_URL in .env after rotation


COST ESTIMATION (Azure PostgreSQL)
===================================

Basic Tier (Development):
  - 1 vCore, 2GB RAM
  - ~$25/month
  - Suitable for < 1000 scans/day

General Purpose (Production):
  - 2 vCores, 8GB RAM
  - ~$150/month
  - Suitable for < 10,000 scans/day

Storage:
  - $0.115/GB/month
  - Estimate: 1GB = ~100,000 scan logs


NEXT STEPS
==========

1. ☐ Set up Azure PostgreSQL server
2. ☐ Test connection with db_test.py
3. ☐ Copy example files to backend/
4. ☐ Update main.py with database integration
5. ☐ Test with a few scans
6. ☐ Query results from Azure Portal
7. ☐ Add analytics dashboard (optional)
8. ☐ Set up automated backups in Azure
"""
