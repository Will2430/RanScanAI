# Quick Start Guide - CNN Model Service

## âœ… Setup Complete!

All dependencies have been installed in your `torch-gpu` conda environment.

## ðŸš€ How to Run

### Step 1: Start Model Service (Terminal 1)

```powershell
# Use the torch-gpu Python directly
& "C:\Users\User\anaconda3\envs\torch-gpu\python.exe" model_service.py
```

**Expected output:**
```
ðŸš€ Starting CNN Model Service...
Loading model from C:/Users/User/OneDrive/Test/K/models/cnn_zenodo_*.keras
âœ“ Model loaded successfully
âœ… CNN Model Service ready!
```

Service will run on: **http://127.0.0.1:8001**

### Step 2: Start Main API (Terminal 2)

```powershell
# Set environment variable to use CNN
$env:USE_CNN_MODEL = "true"

# Run main API (Python 3.14)
python main.py
```

**Expected output:**
```
ðŸš€ Starting SecureGuard Backend...
Connecting to CNN model service...
âœ“ Connected to CNN model service at http://127.0.0.1:8001
âœ… SecureGuard Backend ready!
```

Main API will run on: **http://127.0.0.1:8000**

## ðŸ§ª Test It

### Test Model Service Directly

```powershell
# Health check
curl http://127.0.0.1:8001/health

# Test prediction (upload a file)
curl -X POST -F "file=@C:\path\to\your\file.exe" http://127.0.0.1:8001/predict/bytes
```

### Test Main API

```powershell
# Scan a file
curl -X POST http://127.0.0.1:8000/scan `
  -H "Content-Type: application/json" `
  -d '{\"file_path\": \"C:/path/to/file.exe\"}'
```

## ðŸ“‹ Architecture

```
Python 3.14                          Python 3.10 (torch-gpu)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  main.py     â”‚  HTTP Request      â”‚  model_service.py  â”‚
â”‚  (Port 8000) â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  (Port 8001)       â”‚
â”‚              â”‚                    â”‚                    â”‚
â”‚ CNNClient    â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  TensorFlow Model  â”‚
â”‚              â”‚  HTTP Response     â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”§ Troubleshooting

### Model Service Won't Start

**Check if model exists:**
```powershell
dir C:\Users\User\OneDrive\Test\K\models\cnn_zenodo_*.keras
```

**Verify Python version:**
```powershell
& "C:\Users\User\anaconda3\envs\torch-gpu\python.exe" --version
# Should show: Python 3.10.x
```

**Test TensorFlow:**
```powershell
& "C:\Users\User\anaconda3\envs\torch-gpu\python.exe" -c "import tensorflow; print(tensorflow.__version__)"
```

### Main API Can't Connect

**Check if model service is running:**
```powershell
curl http://127.0.0.1:8001/health
```

**Verify environment variable:**
```powershell
echo $env:USE_CNN_MODEL
# Should output: true
```

### Port Already in Use

**Change model service port** in `model_service.py`:
```python
uvicorn.run(
    "model_service:app",
    port=8002,  # Change this
    # ...
)
```

Then update main API:
```powershell
$env:CNN_MODEL_SERVICE_URL = "http://127.0.0.1:8002"
```

## ðŸ“ Important Notes

- âœ… **NO need to activate conda** - we use the full Python path
- âœ… **Both services must run** - start model service first
- âœ… **Different Python versions** - no conflicts!
- âœ… **Keep both terminals open** while using the system

## ðŸŽ¯ Next Steps

1. âœ… Dependencies installed
2. â–¶ï¸ Start model service (Terminal 1)
3. â–¶ï¸ Start main API (Terminal 2)
4. ðŸ§ª Test with a file
5. ðŸŽ‰ Enjoy zero dependency conflicts!
